== Physical Plan ==
TakeOrderedAndProject (50)
+- * HashAggregate (49)
   +- Exchange (48)
      +- * HashAggregate (47)
         +- * Project (46)
            +- * BroadcastHashJoin Inner BuildRight (45)
               :- * Project (40)
               :  +- * BroadcastHashJoin Inner BuildRight (39)
               :     :- * Project (34)
               :     :  +- * Filter (33)
               :     :     +- * BroadcastHashJoin ExistenceJoin(exists#1) BuildRight (32)
               :     :        :- * BroadcastHashJoin ExistenceJoin(exists#2) BuildRight (22)
               :     :        :  :- * ColumnarToRow (12)
               :     :        :  :  +- CometBroadcastHashJoin (11)
               :     :        :  :     :- CometFilter (2)
               :     :        :  :     :  +- CometScan parquet spark_catalog.default.customer (1)
               :     :        :  :     +- CometBroadcastExchange (10)
               :     :        :  :        +- CometProject (9)
               :     :        :  :           +- CometBroadcastHashJoin (8)
               :     :        :  :              :- CometScan parquet spark_catalog.default.store_sales (3)
               :     :        :  :              +- CometBroadcastExchange (7)
               :     :        :  :                 +- CometProject (6)
               :     :        :  :                    +- CometFilter (5)
               :     :        :  :                       +- CometScan parquet spark_catalog.default.date_dim (4)
               :     :        :  +- BroadcastExchange (21)
               :     :        :     +- * ColumnarToRow (20)
               :     :        :        +- CometProject (19)
               :     :        :           +- CometBroadcastHashJoin (18)
               :     :        :              :- CometScan parquet spark_catalog.default.web_sales (13)
               :     :        :              +- CometBroadcastExchange (17)
               :     :        :                 +- CometProject (16)
               :     :        :                    +- CometFilter (15)
               :     :        :                       +- CometScan parquet spark_catalog.default.date_dim (14)
               :     :        +- BroadcastExchange (31)
               :     :           +- * ColumnarToRow (30)
               :     :              +- CometProject (29)
               :     :                 +- CometBroadcastHashJoin (28)
               :     :                    :- CometScan parquet spark_catalog.default.catalog_sales (23)
               :     :                    +- CometBroadcastExchange (27)
               :     :                       +- CometProject (26)
               :     :                          +- CometFilter (25)
               :     :                             +- CometScan parquet spark_catalog.default.date_dim (24)
               :     +- BroadcastExchange (38)
               :        +- * ColumnarToRow (37)
               :           +- CometFilter (36)
               :              +- CometScan parquet spark_catalog.default.customer_address (35)
               +- BroadcastExchange (44)
                  +- * ColumnarToRow (43)
                     +- CometFilter (42)
                        +- CometScan parquet spark_catalog.default.customer_demographics (41)


(1) Scan parquet spark_catalog.default.customer
Output [3]: [c_customer_sk#3, c_current_cdemo_sk#4, c_current_addr_sk#5]
Batched: true
Location [not included in comparison]/{warehouse_dir}/customer]
PushedFilters: [IsNotNull(c_current_addr_sk), IsNotNull(c_current_cdemo_sk)]
ReadSchema: struct<c_customer_sk:int,c_current_cdemo_sk:int,c_current_addr_sk:int>

(2) CometFilter
Input [3]: [c_customer_sk#3, c_current_cdemo_sk#4, c_current_addr_sk#5]
Condition : (isnotnull(c_current_addr_sk#5) AND isnotnull(c_current_cdemo_sk#4))

(3) Scan parquet spark_catalog.default.store_sales
Output [2]: [ss_customer_sk#6, ss_sold_date_sk#7]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(ss_sold_date_sk#7), dynamicpruningexpression(ss_sold_date_sk#7 IN dynamicpruning#8)]
ReadSchema: struct<ss_customer_sk:int>

(4) Scan parquet spark_catalog.default.date_dim
Output [3]: [d_date_sk#9, d_year#10, d_qoy#11]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_year), IsNotNull(d_qoy), EqualTo(d_year,2002), LessThan(d_qoy,4), IsNotNull(d_date_sk)]
ReadSchema: struct<d_date_sk:int,d_year:int,d_qoy:int>

(5) CometFilter
Input [3]: [d_date_sk#9, d_year#10, d_qoy#11]
Condition : ((((isnotnull(d_year#10) AND isnotnull(d_qoy#11)) AND (d_year#10 = 2002)) AND (d_qoy#11 < 4)) AND isnotnull(d_date_sk#9))

(6) CometProject
Input [3]: [d_date_sk#9, d_year#10, d_qoy#11]
Arguments: [d_date_sk#9], [d_date_sk#9]

(7) CometBroadcastExchange
Input [1]: [d_date_sk#9]
Arguments: [d_date_sk#9]

(8) CometBroadcastHashJoin
Left output [2]: [ss_customer_sk#6, ss_sold_date_sk#7]
Right output [1]: [d_date_sk#9]
Arguments: [ss_sold_date_sk#7], [d_date_sk#9], Inner

(9) CometProject
Input [3]: [ss_customer_sk#6, ss_sold_date_sk#7, d_date_sk#9]
Arguments: [ss_customer_sk#6], [ss_customer_sk#6]

(10) CometBroadcastExchange
Input [1]: [ss_customer_sk#6]
Arguments: [ss_customer_sk#6]

(11) CometBroadcastHashJoin
Left output [3]: [c_customer_sk#3, c_current_cdemo_sk#4, c_current_addr_sk#5]
Right output [1]: [ss_customer_sk#6]
Arguments: [c_customer_sk#3], [ss_customer_sk#6], LeftSemi

(12) ColumnarToRow [codegen id : 5]
Input [3]: [c_customer_sk#3, c_current_cdemo_sk#4, c_current_addr_sk#5]

(13) Scan parquet spark_catalog.default.web_sales
Output [2]: [ws_bill_customer_sk#12, ws_sold_date_sk#13]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(ws_sold_date_sk#13), dynamicpruningexpression(ws_sold_date_sk#13 IN dynamicpruning#14)]
ReadSchema: struct<ws_bill_customer_sk:int>

(14) Scan parquet spark_catalog.default.date_dim
Output [3]: [d_date_sk#15, d_year#16, d_qoy#17]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_year), IsNotNull(d_qoy), EqualTo(d_year,2002), LessThan(d_qoy,4), IsNotNull(d_date_sk)]
ReadSchema: struct<d_date_sk:int,d_year:int,d_qoy:int>

(15) CometFilter
Input [3]: [d_date_sk#15, d_year#16, d_qoy#17]
Condition : ((((isnotnull(d_year#16) AND isnotnull(d_qoy#17)) AND (d_year#16 = 2002)) AND (d_qoy#17 < 4)) AND isnotnull(d_date_sk#15))

(16) CometProject
Input [3]: [d_date_sk#15, d_year#16, d_qoy#17]
Arguments: [d_date_sk#15], [d_date_sk#15]

(17) CometBroadcastExchange
Input [1]: [d_date_sk#15]
Arguments: [d_date_sk#15]

(18) CometBroadcastHashJoin
Left output [2]: [ws_bill_customer_sk#12, ws_sold_date_sk#13]
Right output [1]: [d_date_sk#15]
Arguments: [ws_sold_date_sk#13], [d_date_sk#15], Inner

(19) CometProject
Input [3]: [ws_bill_customer_sk#12, ws_sold_date_sk#13, d_date_sk#15]
Arguments: [ws_bill_customer_sk#12], [ws_bill_customer_sk#12]

(20) ColumnarToRow [codegen id : 1]
Input [1]: [ws_bill_customer_sk#12]

(21) BroadcastExchange
Input [1]: [ws_bill_customer_sk#12]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=1]

(22) BroadcastHashJoin [codegen id : 5]
Left keys [1]: [c_customer_sk#3]
Right keys [1]: [ws_bill_customer_sk#12]
Join type: ExistenceJoin(exists#2)
Join condition: None

(23) Scan parquet spark_catalog.default.catalog_sales
Output [2]: [cs_ship_customer_sk#18, cs_sold_date_sk#19]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(cs_sold_date_sk#19), dynamicpruningexpression(cs_sold_date_sk#19 IN dynamicpruning#20)]
ReadSchema: struct<cs_ship_customer_sk:int>

(24) Scan parquet spark_catalog.default.date_dim
Output [3]: [d_date_sk#21, d_year#22, d_qoy#23]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_year), IsNotNull(d_qoy), EqualTo(d_year,2002), LessThan(d_qoy,4), IsNotNull(d_date_sk)]
ReadSchema: struct<d_date_sk:int,d_year:int,d_qoy:int>

(25) CometFilter
Input [3]: [d_date_sk#21, d_year#22, d_qoy#23]
Condition : ((((isnotnull(d_year#22) AND isnotnull(d_qoy#23)) AND (d_year#22 = 2002)) AND (d_qoy#23 < 4)) AND isnotnull(d_date_sk#21))

(26) CometProject
Input [3]: [d_date_sk#21, d_year#22, d_qoy#23]
Arguments: [d_date_sk#21], [d_date_sk#21]

(27) CometBroadcastExchange
Input [1]: [d_date_sk#21]
Arguments: [d_date_sk#21]

(28) CometBroadcastHashJoin
Left output [2]: [cs_ship_customer_sk#18, cs_sold_date_sk#19]
Right output [1]: [d_date_sk#21]
Arguments: [cs_sold_date_sk#19], [d_date_sk#21], Inner

(29) CometProject
Input [3]: [cs_ship_customer_sk#18, cs_sold_date_sk#19, d_date_sk#21]
Arguments: [cs_ship_customer_sk#18], [cs_ship_customer_sk#18]

(30) ColumnarToRow [codegen id : 2]
Input [1]: [cs_ship_customer_sk#18]

(31) BroadcastExchange
Input [1]: [cs_ship_customer_sk#18]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=2]

(32) BroadcastHashJoin [codegen id : 5]
Left keys [1]: [c_customer_sk#3]
Right keys [1]: [cs_ship_customer_sk#18]
Join type: ExistenceJoin(exists#1)
Join condition: None

(33) Filter [codegen id : 5]
Input [5]: [c_customer_sk#3, c_current_cdemo_sk#4, c_current_addr_sk#5, exists#2, exists#1]
Condition : (exists#2 OR exists#1)

(34) Project [codegen id : 5]
Output [2]: [c_current_cdemo_sk#4, c_current_addr_sk#5]
Input [5]: [c_customer_sk#3, c_current_cdemo_sk#4, c_current_addr_sk#5, exists#2, exists#1]

(35) Scan parquet spark_catalog.default.customer_address
Output [2]: [ca_address_sk#24, ca_state#25]
Batched: true
Location [not included in comparison]/{warehouse_dir}/customer_address]
PushedFilters: [IsNotNull(ca_address_sk)]
ReadSchema: struct<ca_address_sk:int,ca_state:string>

(36) CometFilter
Input [2]: [ca_address_sk#24, ca_state#25]
Condition : isnotnull(ca_address_sk#24)

(37) ColumnarToRow [codegen id : 3]
Input [2]: [ca_address_sk#24, ca_state#25]

(38) BroadcastExchange
Input [2]: [ca_address_sk#24, ca_state#25]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=3]

(39) BroadcastHashJoin [codegen id : 5]
Left keys [1]: [c_current_addr_sk#5]
Right keys [1]: [ca_address_sk#24]
Join type: Inner
Join condition: None

(40) Project [codegen id : 5]
Output [2]: [c_current_cdemo_sk#4, ca_state#25]
Input [4]: [c_current_cdemo_sk#4, c_current_addr_sk#5, ca_address_sk#24, ca_state#25]

(41) Scan parquet spark_catalog.default.customer_demographics
Output [6]: [cd_demo_sk#26, cd_gender#27, cd_marital_status#28, cd_dep_count#29, cd_dep_employed_count#30, cd_dep_college_count#31]
Batched: true
Location [not included in comparison]/{warehouse_dir}/customer_demographics]
PushedFilters: [IsNotNull(cd_demo_sk)]
ReadSchema: struct<cd_demo_sk:int,cd_gender:string,cd_marital_status:string,cd_dep_count:int,cd_dep_employed_count:int,cd_dep_college_count:int>

(42) CometFilter
Input [6]: [cd_demo_sk#26, cd_gender#27, cd_marital_status#28, cd_dep_count#29, cd_dep_employed_count#30, cd_dep_college_count#31]
Condition : isnotnull(cd_demo_sk#26)

(43) ColumnarToRow [codegen id : 4]
Input [6]: [cd_demo_sk#26, cd_gender#27, cd_marital_status#28, cd_dep_count#29, cd_dep_employed_count#30, cd_dep_college_count#31]

(44) BroadcastExchange
Input [6]: [cd_demo_sk#26, cd_gender#27, cd_marital_status#28, cd_dep_count#29, cd_dep_employed_count#30, cd_dep_college_count#31]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=4]

(45) BroadcastHashJoin [codegen id : 5]
Left keys [1]: [c_current_cdemo_sk#4]
Right keys [1]: [cd_demo_sk#26]
Join type: Inner
Join condition: None

(46) Project [codegen id : 5]
Output [6]: [ca_state#25, cd_gender#27, cd_marital_status#28, cd_dep_count#29, cd_dep_employed_count#30, cd_dep_college_count#31]
Input [8]: [c_current_cdemo_sk#4, ca_state#25, cd_demo_sk#26, cd_gender#27, cd_marital_status#28, cd_dep_count#29, cd_dep_employed_count#30, cd_dep_college_count#31]

(47) HashAggregate [codegen id : 5]
Input [6]: [ca_state#25, cd_gender#27, cd_marital_status#28, cd_dep_count#29, cd_dep_employed_count#30, cd_dep_college_count#31]
Keys [6]: [ca_state#25, cd_gender#27, cd_marital_status#28, cd_dep_count#29, cd_dep_employed_count#30, cd_dep_college_count#31]
Functions [10]: [partial_count(1), partial_min(cd_dep_count#29), partial_max(cd_dep_count#29), partial_avg(cd_dep_count#29), partial_min(cd_dep_employed_count#30), partial_max(cd_dep_employed_count#30), partial_avg(cd_dep_employed_count#30), partial_min(cd_dep_college_count#31), partial_max(cd_dep_college_count#31), partial_avg(cd_dep_college_count#31)]
Aggregate Attributes [13]: [count#32, min#33, max#34, sum#35, count#36, min#37, max#38, sum#39, count#40, min#41, max#42, sum#43, count#44]
Results [19]: [ca_state#25, cd_gender#27, cd_marital_status#28, cd_dep_count#29, cd_dep_employed_count#30, cd_dep_college_count#31, count#45, min#46, max#47, sum#48, count#49, min#50, max#51, sum#52, count#53, min#54, max#55, sum#56, count#57]

(48) Exchange
Input [19]: [ca_state#25, cd_gender#27, cd_marital_status#28, cd_dep_count#29, cd_dep_employed_count#30, cd_dep_college_count#31, count#45, min#46, max#47, sum#48, count#49, min#50, max#51, sum#52, count#53, min#54, max#55, sum#56, count#57]
Arguments: hashpartitioning(ca_state#25, cd_gender#27, cd_marital_status#28, cd_dep_count#29, cd_dep_employed_count#30, cd_dep_college_count#31, 5), ENSURE_REQUIREMENTS, [plan_id=5]

(49) HashAggregate [codegen id : 6]
Input [19]: [ca_state#25, cd_gender#27, cd_marital_status#28, cd_dep_count#29, cd_dep_employed_count#30, cd_dep_college_count#31, count#45, min#46, max#47, sum#48, count#49, min#50, max#51, sum#52, count#53, min#54, max#55, sum#56, count#57]
Keys [6]: [ca_state#25, cd_gender#27, cd_marital_status#28, cd_dep_count#29, cd_dep_employed_count#30, cd_dep_college_count#31]
Functions [10]: [count(1), min(cd_dep_count#29), max(cd_dep_count#29), avg(cd_dep_count#29), min(cd_dep_employed_count#30), max(cd_dep_employed_count#30), avg(cd_dep_employed_count#30), min(cd_dep_college_count#31), max(cd_dep_college_count#31), avg(cd_dep_college_count#31)]
Aggregate Attributes [10]: [count(1)#58, min(cd_dep_count#29)#59, max(cd_dep_count#29)#60, avg(cd_dep_count#29)#61, min(cd_dep_employed_count#30)#62, max(cd_dep_employed_count#30)#63, avg(cd_dep_employed_count#30)#64, min(cd_dep_college_count#31)#65, max(cd_dep_college_count#31)#66, avg(cd_dep_college_count#31)#67]
Results [18]: [ca_state#25, cd_gender#27, cd_marital_status#28, count(1)#58 AS cnt1#68, min(cd_dep_count#29)#59 AS min(cd_dep_count)#69, max(cd_dep_count#29)#60 AS max(cd_dep_count)#70, avg(cd_dep_count#29)#61 AS avg(cd_dep_count)#71, cd_dep_employed_count#30, count(1)#58 AS cnt2#72, min(cd_dep_employed_count#30)#62 AS min(cd_dep_employed_count)#73, max(cd_dep_employed_count#30)#63 AS max(cd_dep_employed_count)#74, avg(cd_dep_employed_count#30)#64 AS avg(cd_dep_employed_count)#75, cd_dep_college_count#31, count(1)#58 AS cnt3#76, min(cd_dep_college_count#31)#65 AS min(cd_dep_college_count)#77, max(cd_dep_college_count#31)#66 AS max(cd_dep_college_count)#78, avg(cd_dep_college_count#31)#67 AS avg(cd_dep_college_count)#79, cd_dep_count#29]

(50) TakeOrderedAndProject
Input [18]: [ca_state#25, cd_gender#27, cd_marital_status#28, cnt1#68, min(cd_dep_count)#69, max(cd_dep_count)#70, avg(cd_dep_count)#71, cd_dep_employed_count#30, cnt2#72, min(cd_dep_employed_count)#73, max(cd_dep_employed_count)#74, avg(cd_dep_employed_count)#75, cd_dep_college_count#31, cnt3#76, min(cd_dep_college_count)#77, max(cd_dep_college_count)#78, avg(cd_dep_college_count)#79, cd_dep_count#29]
Arguments: 100, [ca_state#25 ASC NULLS FIRST, cd_gender#27 ASC NULLS FIRST, cd_marital_status#28 ASC NULLS FIRST, cd_dep_count#29 ASC NULLS FIRST, cd_dep_employed_count#30 ASC NULLS FIRST, cd_dep_college_count#31 ASC NULLS FIRST], [ca_state#25, cd_gender#27, cd_marital_status#28, cnt1#68, min(cd_dep_count)#69, max(cd_dep_count)#70, avg(cd_dep_count)#71, cd_dep_employed_count#30, cnt2#72, min(cd_dep_employed_count)#73, max(cd_dep_employed_count)#74, avg(cd_dep_employed_count)#75, cd_dep_college_count#31, cnt3#76, min(cd_dep_college_count)#77, max(cd_dep_college_count)#78, avg(cd_dep_college_count)#79]

===== Subqueries =====

Subquery:1 Hosting operator id = 3 Hosting Expression = ss_sold_date_sk#7 IN dynamicpruning#8
BroadcastExchange (55)
+- * ColumnarToRow (54)
   +- CometProject (53)
      +- CometFilter (52)
         +- CometScan parquet spark_catalog.default.date_dim (51)


(51) Scan parquet spark_catalog.default.date_dim
Output [3]: [d_date_sk#9, d_year#10, d_qoy#11]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_year), IsNotNull(d_qoy), EqualTo(d_year,2002), LessThan(d_qoy,4), IsNotNull(d_date_sk)]
ReadSchema: struct<d_date_sk:int,d_year:int,d_qoy:int>

(52) CometFilter
Input [3]: [d_date_sk#9, d_year#10, d_qoy#11]
Condition : ((((isnotnull(d_year#10) AND isnotnull(d_qoy#11)) AND (d_year#10 = 2002)) AND (d_qoy#11 < 4)) AND isnotnull(d_date_sk#9))

(53) CometProject
Input [3]: [d_date_sk#9, d_year#10, d_qoy#11]
Arguments: [d_date_sk#9], [d_date_sk#9]

(54) ColumnarToRow [codegen id : 1]
Input [1]: [d_date_sk#9]

(55) BroadcastExchange
Input [1]: [d_date_sk#9]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=6]

Subquery:2 Hosting operator id = 13 Hosting Expression = ws_sold_date_sk#13 IN dynamicpruning#8

Subquery:3 Hosting operator id = 23 Hosting Expression = cs_sold_date_sk#19 IN dynamicpruning#8


